{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9iqewdWEIxq"
      },
      "source": [
        "Nom 1: Eric Escrich\n",
        "\n",
        "Nom 2: Adrià Vilariño"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31QNYpMU6k4W"
      },
      "source": [
        "# Repte 4: Fashion CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UkgQ3eg6sNh"
      },
      "source": [
        "## Enunciat\n",
        "\n",
        "En aquest repte classficarem les diferents peces de roba fent servir una **Xarxa Neuronal Convolucional**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y2LuZSRs6UFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6026d51e-6de0-4f50-a2ab-91d3a5f4b0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Descarrega i prepara les dades del dataset fashion_mnist. En aquest cas, no\n",
        "# transformarem la sortida del model a un vector de 0' i 1's, per tant, haurem\n",
        "# d'escollir bé la funció de pèrdua a l'hora de fer l'entrenament de la xarxa.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalitzem dataset\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiQoIl917qOO"
      },
      "outputs": [],
      "source": [
        "# Crea una Xarxa Neuronal amb 2 capes Convolucionals de 64 i 128 filtres, de\n",
        "# mides 7x7 i 3x3. A cada capa li aplicarem la operació MaxPooling 2x2. Afegeix\n",
        "# també una capa densa de 64 neurones abans de la capa de sortida. Experimenta\n",
        "# amb les diferents opcions de l'hiperparàmetre padding.\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Primera capa convolucional amb 64 filtres de mida 7x7 i padding 'same'\n",
        "model.add(Conv2D(64, kernel_size=(7, 7), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Segona capa convolucional amb 128 filtres de mida 3x3 i padding 'same'\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Aplanar les dades per a les capes denses\n",
        "model.add(Flatten())\n",
        "\n",
        "# Capa densa amb 64 neurones\n",
        "model.add(Dense(64))\n",
        "\n",
        "# Capa de sortida amb 10 neurones (una per cada classe) i activació softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compilar el model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEMr9fKk9Nip",
        "outputId": "06a358a1-e7e6-4049-9399-2c1493f2e213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7956 - loss: 0.5671 - val_accuracy: 0.8848 - val_loss: 0.3175\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.8973 - loss: 0.2836 - val_accuracy: 0.8931 - val_loss: 0.2876\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9106 - loss: 0.2478 - val_accuracy: 0.9063 - val_loss: 0.2635\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9209 - loss: 0.2154 - val_accuracy: 0.9034 - val_loss: 0.2722\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9305 - loss: 0.1906 - val_accuracy: 0.9108 - val_loss: 0.2488\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9381 - loss: 0.1700 - val_accuracy: 0.9118 - val_loss: 0.2492\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9439 - loss: 0.1508 - val_accuracy: 0.9053 - val_loss: 0.2776\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9499 - loss: 0.1381 - val_accuracy: 0.9149 - val_loss: 0.2617\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9533 - loss: 0.1259 - val_accuracy: 0.9119 - val_loss: 0.2806\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9587 - loss: 0.1130 - val_accuracy: 0.9133 - val_loss: 0.2731\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9012 - loss: 0.3118\n",
            "Pèrdua del model: 0.3013814389705658\n",
            "Precisió del model: 0.9065999984741211\n"
          ]
        }
      ],
      "source": [
        "# Entrena el model i visualitza el resultat de l'evaluació amb les dades de validació\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Avaluar el model amb les dades de test\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Pèrdua del model: {loss}\")\n",
        "print(f\"Precisió del model: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbbKgK5T-rUQ"
      },
      "source": [
        "La capa **BatchNormalization** normalitza les entrades de la capa de manera que la mitja de la seva activació de sortida sigui 0 i la desviació estàndard 1.\n",
        "\n",
        "La capa **Dropout** ajuda a evitar els error per sobreentrenament (overfitting). Aquesta tècnica es basa en ignorar certs conjunts de neurones de manera aleatòria durant la fase d'entrenament."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyQkzhIs-fjR",
        "outputId": "9928a8e4-d2f3-41b6-d804-fee8c4870e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 204ms/step - accuracy: 0.7565 - loss: 0.7342 - val_accuracy: 0.8821 - val_loss: 0.3209\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 200ms/step - accuracy: 0.8784 - loss: 0.3483 - val_accuracy: 0.9078 - val_loss: 0.2559\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 199ms/step - accuracy: 0.8961 - loss: 0.2914 - val_accuracy: 0.9147 - val_loss: 0.2296\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 198ms/step - accuracy: 0.9063 - loss: 0.2651 - val_accuracy: 0.9192 - val_loss: 0.2175\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 199ms/step - accuracy: 0.9192 - loss: 0.2290 - val_accuracy: 0.9248 - val_loss: 0.2183\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 199ms/step - accuracy: 0.9307 - loss: 0.2006 - val_accuracy: 0.9257 - val_loss: 0.2072\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 199ms/step - accuracy: 0.9355 - loss: 0.1781 - val_accuracy: 0.9277 - val_loss: 0.2085\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 199ms/step - accuracy: 0.9442 - loss: 0.1541 - val_accuracy: 0.9304 - val_loss: 0.2116\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 200ms/step - accuracy: 0.9522 - loss: 0.1366 - val_accuracy: 0.9312 - val_loss: 0.2087\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 200ms/step - accuracy: 0.9587 - loss: 0.1169 - val_accuracy: 0.9333 - val_loss: 0.2096\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.9321 - loss: 0.2291\n",
            "Pèrdua del model: 0.22301290929317474\n",
            "Precisió del model: 0.9308000206947327\n"
          ]
        }
      ],
      "source": [
        "# Crea i entrena una CNN amb 4 capes convolucionals de 32, 32, 64 i 128 filtres\n",
        "# i mida de 3x3. En comptes de la operació MaxPooling, aplicarem les capes\n",
        "# BatchNormalization i Dropout al 25% a cadascuna d'elles. Configurarem el padding\n",
        "# de manera que la sortida i l'entrada siguin de la mateixa mida.\n",
        "\n",
        "# Per finalitzar la nostra xarxa, afegirem 2 capes denses de 512 i 128 neurones\n",
        "# amb les capes BatchNormalization i Dropout al 50% abans de la capa de sortida.\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Primera capa convolucional amb 32 filtres de mida 3x3 i padding 'same'\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Segona capa convolucional amb 32 filtres de mida 3x3 i padding 'same'\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Tercera capa convolucional amb 64 filtres de mida 3x3 i padding 'same'\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Quarta capa convolucional amb 128 filtres de mida 3x3 i padding 'same'\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Aplanar les dades per a les capes denses\n",
        "model.add(Flatten())\n",
        "\n",
        "# Primera capa densa amb 512 neurones\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Segona capa densa amb 128 neurones\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Capa de sortida amb 10 neurones (una per cada classe) i activació softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compilar el model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Avaluar el model amb les dades de test\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Pèrdua del model: {loss}\")\n",
        "print(f\"Precisió del model: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0aagAMDYw5"
      },
      "source": [
        "## Part opcional\n",
        "\n",
        "Experimenta amb diferents configuracions de xarxa modificant el número de capes, morfologia, hiperparàmetres, etc i compara els resultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IyiNJDFXDpwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a873ed41-4942-47fb-ffbf-b34bd031bccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 37ms/step - accuracy: 0.7948 - loss: 0.6319 - val_accuracy: 0.8973 - val_loss: 0.2829\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 36ms/step - accuracy: 0.8960 - loss: 0.3029 - val_accuracy: 0.9156 - val_loss: 0.2243\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9176 - loss: 0.2358 - val_accuracy: 0.9167 - val_loss: 0.2256\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9276 - loss: 0.2037 - val_accuracy: 0.9262 - val_loss: 0.2013\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 37ms/step - accuracy: 0.9425 - loss: 0.1676 - val_accuracy: 0.9265 - val_loss: 0.2060\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 36ms/step - accuracy: 0.9526 - loss: 0.1363 - val_accuracy: 0.9292 - val_loss: 0.2022\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9613 - loss: 0.1112 - val_accuracy: 0.9343 - val_loss: 0.1999\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9682 - loss: 0.0922 - val_accuracy: 0.9347 - val_loss: 0.2112\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9766 - loss: 0.0703 - val_accuracy: 0.9300 - val_loss: 0.2337\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 36ms/step - accuracy: 0.9793 - loss: 0.0611 - val_accuracy: 0.9310 - val_loss: 0.2271\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9318 - loss: 0.2331\n",
            "Pèrdua del model: 0.2270851582288742\n",
            "Precisió del model: 0.9309999942779541\n"
          ]
        }
      ],
      "source": [
        "optional_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optional_model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el model\n",
        "history = optional_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Avaluar el model amb les dades de test\n",
        "loss, accuracy = optional_model.evaluate(x_test, y_test)\n",
        "print(f\"Pèrdua del model: {loss}\")\n",
        "print(f\"Precisió del model: {accuracy}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}